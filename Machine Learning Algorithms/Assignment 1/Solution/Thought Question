Question 1 : The second chapter of the notes gives an introduction to maximum likelihood estimation and using this we were estimating parameters for the distribution. It occured to me does M.L.E. always gaurantees to give the best result for a parameter? How do we know that we are not stuck at some local maxima, if say, we are dealing with a bimodal distribution?

Question 2: In the section where we talk about bayes risk classifier we associate cost with the prediction, I think it's bit tricky (may even be not necessary) to come up with a cost for different classifiers as we need to penalize a binary classifier differently, a multiclass or multilabel in another way. My question is does associating a cost help us correcting our predictions and really give us more accurate model? why don't we just remove it from the picture by lowering the proportion of training data and which will decrease the misclassifications and we get a more accurate model and then we do not even have to bother with the associating a cost with our function anymore.

Question 3: In first chapter, we tried to build a foundation of machine learning by starting with different types of distribution and we hypothesise the type of distribution as our probable model of the data and then estimating it's parameter using M.L.E. Since we have a luxury of chosing a model from the infinite hypothesis set. It raises a question that why don't we take lots of models, make estimates from them and at the end average the results. This should minimize the error rate. shouldn't it?
